{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama Fine-tuning Configuration with LLaMA-Factory\n",
    "\n",
    "This notebook presents the configuration for fine-tuning Llama models on ICD-10 coding tasks using LLaMA-Factory, including both training and evaluation configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment\n",
    "\n",
    "First, let's make sure LLaMA-Factory is properly installed. If not already installed, uncomment and run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/hiyouga/LLaMA-Factory.git\n",
    "# %cd LLaMA-Factory\n",
    "# !pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Fine-tuning Configuration\n",
    "\n",
    "The configuration for the initial fine-tuning stage using the complete ICD-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create initial fine-tuning config file\n",
    "initial_finetuning_config = \"\"\"\n",
    "### model\n",
    "model_name_or_path: Llama-3.2-1B-Instruct\n",
    "\n",
    "### method\n",
    "stage: sft\n",
    "do_train: true\n",
    "finetuning_type: full\n",
    "deepspeed: examples/deepspeed/ds_z2_config.json\n",
    "\n",
    "### dataset\n",
    "dataset: full_free_icd_train\n",
    "template: llama3\n",
    "cutoff_len: 5000\n",
    "max_samples: 60000\n",
    "overwrite_cache: true\n",
    "preprocessing_num_workers: 1\n",
    "\n",
    "### output\n",
    "output_dir: saves/Llama-3.2-1B-Instruct/full/final/initial_finetuning\n",
    "logging_steps: 10\n",
    "save_steps: 2000000\n",
    "plot_loss: true\n",
    "overwrite_output_dir: true\n",
    "\n",
    "### train\n",
    "per_device_train_batch_size: 2\n",
    "gradient_accumulation_steps: 1\n",
    "learning_rate: 1.0e-5\n",
    "num_train_epochs: 11.0\n",
    "lr_scheduler_type: cosine\n",
    "warmup_ratio: 0.1\n",
    "bf16: true\n",
    "ddp_timeout: 180000000\n",
    "\n",
    "### eval\n",
    "val_size: 0.1\n",
    "per_device_eval_batch_size: 1\n",
    "eval_strategy: steps\n",
    "eval_steps: 20000\n",
    "\"\"\"\n",
    "\n",
    "# Write to file\n",
    "with open(\"initial_finetuning_config.yaml\", \"w\") as f:\n",
    "    f.write(initial_finetuning_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Fine-tuning Configuration\n",
    "\n",
    "Configuration for the enhanced fine-tuning stage targeting linguistic and lexical variations. This configuration continues training from the initially fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create enhanced fine-tuning config file\n",
    "enhanced_finetuning_config = \"\"\"\n",
    "### model\n",
    "model_name_or_path: saves/Llama-3.2-1B-Instruct/full/final/initial_finetuning\n",
    "\n",
    "### method\n",
    "stage: sft\n",
    "do_train: true\n",
    "finetuning_type: full\n",
    "deepspeed: examples/deepspeed/ds_z2_config.json\n",
    "\n",
    "### dataset\n",
    "dataset: llama_abb_enhance_data_10,llama_multi_enhance_data_10,llama_typo_enhance_data_10,llama_sentence_enhance_data_10,llama_reorder_enhance_data_10\n",
    "template: llama3\n",
    "cutoff_len: 5000\n",
    "max_samples: 10000\n",
    "overwrite_cache: true\n",
    "preprocessing_num_workers: 1\n",
    "\n",
    "### output\n",
    "output_dir: saves/Llama-3.2-1B-Instruct/full/final/enhanced_finetuning\n",
    "logging_steps: 10\n",
    "save_steps: 500000\n",
    "plot_loss: true\n",
    "overwrite_output_dir: true\n",
    "\n",
    "### train\n",
    "per_device_train_batch_size: 2\n",
    "gradient_accumulation_steps: 1\n",
    "learning_rate: 5.0e-6  # Lower learning rate for enhanced tuning\n",
    "num_train_epochs: 5.0  # Fewer epochs for enhanced tuning\n",
    "lr_scheduler_type: cosine\n",
    "warmup_ratio: 0.1\n",
    "bf16: true\n",
    "ddp_timeout: 180000000\n",
    "\n",
    "### eval\n",
    "val_size: 0.1\n",
    "per_device_eval_batch_size: 1\n",
    "eval_strategy: steps\n",
    "eval_steps: 10000\n",
    "\"\"\"\n",
    "\n",
    "# Write to file\n",
    "with open(\"enhanced_finetuning_config.yaml\", \"w\") as f:\n",
    "    f.write(enhanced_finetuning_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Configuration\n",
    "\n",
    "Configuration for evaluating the fine-tuned model on test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluation config file\n",
    "evaluation_config = \"\"\"\n",
    "### model\n",
    "model_name_or_path: saves/Llama-3.2-1B-Instruct/full/final/enhanced_finetuning\n",
    "\n",
    "### method\n",
    "stage: sft\n",
    "do_predict: true\n",
    "finetuning_type: full\n",
    "max_new_tokens: 10000\n",
    "\n",
    "### dataset\n",
    "eval_dataset: full_free_icd_test\n",
    "template: llama3\n",
    "cutoff_len: 150000\n",
    "max_samples: 20000\n",
    "overwrite_cache: true\n",
    "preprocessing_num_workers: 5\n",
    "\n",
    "### output\n",
    "output_dir: saves/Llama-3.2-1B-Instruct/full/final/result/evaluation_results\n",
    "overwrite_output_dir: true\n",
    "\n",
    "### eval\n",
    "per_device_eval_batch_size: 2\n",
    "predict_with_generate: true\n",
    "\"\"\"\n",
    "\n",
    "# Write to file\n",
    "with open(\"evaluation_config.yaml\", \"w\") as f:\n",
    "    f.write(evaluation_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Fine-tuning with LLaMA-Factory\n",
    "\n",
    "Commands to execute the fine-tuning process using LLaMA-Factory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Command for running initial fine-tuning\n",
    "initial_finetuning_cmd = \"python -m llmtuner.cli.run_with_config initial_finetuning_config.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Command for running enhanced fine-tuning\n",
    "enhanced_finetuning_cmd = \"python -m llmtuner.cli.run_with_config enhanced_finetuning_config.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Command for evaluation\n",
    "evaluation_cmd = \"python -m llmtuner.cli.run_with_config evaluation_config.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-GPU Training with Distributed Data Parallel (DDP)\n",
    "\n",
    "For faster training on multiple GPUs using PyTorch's Distributed Data Parallel along with DeepSpeed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-GPU training command with DDP\n",
    "multi_gpu_training_cmd = \"\"\"CUDA_VISIBLE_DEVICES=0,1,2,3 \\\n",
    "python -m torch.distributed.run \\\n",
    "    --nproc_per_node=4 \\\n",
    "    -m llmtuner.cli.run_with_config initial_finetuning_config.yaml\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepSpeed Configuration\n",
    "\n",
    "The DeepSpeed configuration file referenced in the YAML configs (ds_z2_config.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DeepSpeed config file\n",
    "deepspeed_config = \"\"\"\n",
    "{\n",
    "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "    \"gradient_accumulation_steps\": \"auto\",\n",
    "    \"gradient_clipping\": 1.0,\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 2,\n",
    "        \"offload_optimizer\": {\n",
    "            \"device\": \"cpu\"\n",
    "        },\n",
    "        \"contiguous_gradients\": true,\n",
    "        \"overlap_comm\": true\n",
    "    },\n",
    "    \"fp16\": {\n",
    "        \"enabled\": \"auto\",\n",
    "        \"loss_scale\": 0,\n",
    "        \"initial_scale_power\": 16,\n",
    "        \"loss_scale_window\": 1000,\n",
    "        \"hysteresis\": 2,\n",
    "        \"min_loss_scale\": 1\n",
    "    },\n",
    "    \"bf16\": {\n",
    "        \"enabled\": \"auto\"\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"AdamW\",\n",
    "        \"params\": {\n",
    "            \"lr\": \"auto\",\n",
    "            \"betas\": [0.9, 0.999],\n",
    "            \"eps\": 1e-8,\n",
    "            \"weight_decay\": \"auto\"\n",
    "        }\n",
    "    },\n",
    "    \"scheduler\": {\n",
    "        \"type\": \"WarmupDecayLR\",\n",
    "        \"params\": {\n",
    "            \"warmup_min_lr\": \"auto\",\n",
    "            \"warmup_max_lr\": \"auto\",\n",
    "            \"warmup_num_steps\": \"auto\",\n",
    "            \"total_num_steps\": \"auto\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs(\"examples/deepspeed\", exist_ok=True)\n",
    "\n",
    "# Write to file\n",
    "with open(\"examples/deepspeed/ds_z2_config.json\", \"w\") as f:\n",
    "    f.write(deepspeed_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Configuration\n",
    "\n",
    "Instructions for preparing your datasets in the format expected by LLaMA-Factory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of dataset format (JSON Lines)\n",
    "dataset_example = \"\"\"\n",
    "{\n",
    "    \"system\": \"You are a medical coding specialist responsible for assigning ICD-10 codes to clinical documentation\",\n",
    "    \"input\": \"Generate appropriate ICD-10 codes based on standard descriptions: Type 2 diabetes mellitus without complications\",\n",
    "    \"output\": \"E11.9\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "print(\"Dataset example (JSONL format):\")\n",
    "print(dataset_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Configuration Parameters Explained\n",
    "\n",
    "### Two-Stage Fine-tuning Approach\n",
    "\n",
    "1. **Initial Fine-tuning Stage**:\n",
    "   - Uses complete ICD-10 dataset: `dataset: full_free_icd_train`\n",
    "   - Higher learning rate: `learning_rate: 1.0e-5`\n",
    "   - More epochs: `num_train_epochs: 11.0`\n",
    "   - Focuses on building foundational medical coding knowledge\n",
    "\n",
    "2. **Enhanced Fine-tuning Stage**:\n",
    "   - Uses linguistic variation datasets: `dataset: llama_abb_enhance_data_10,...`\n",
    "   - Lower learning rate: `learning_rate: 5.0e-6`\n",
    "   - Fewer epochs: `num_train_epochs: 5.0`\n",
    "   - Builds on initial model: `model_name_or_path: .../initial_finetuning`\n",
    "   - Focuses on handling linguistic and lexical variations\n",
    "\n",
    "### Performance Optimization\n",
    "\n",
    "- **DeepSpeed Integration**: Zero-2 optimization for memory efficiency\n",
    "- **BF16 Precision**: `bf16: true` for faster training without significant precision loss\n",
    "- **Multi-GPU Training**: Using PyTorch DDP for distributed training\n",
    "- **Gradient Accumulation**: For effectively larger batch sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shell Script for Full Training Process\n",
    "\n",
    "This script automates the complete two-stage fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a shell script for the full training process\n",
    "full_training_script = \"\"\"\n",
    "#!/bin/bash\n",
    "\n",
    "# Create needed directories\n",
    "mkdir -p examples/deepspeed\n",
    "mkdir -p saves/Llama-3.2-1B-Instruct/full/final/initial_finetuning\n",
    "mkdir -p saves/Llama-3.2-1B-Instruct/full/final/enhanced_finetuning\n",
    "mkdir -p saves/Llama-3.2-1B-Instruct/full/final/result/evaluation_results\n",
    "\n",
    "# Generate DeepSpeed config\n",
    "cat > examples/deepspeed/ds_z2_config.json << 'EOL'\n",
    "{\n",
    "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "    \"gradient_accumulation_steps\": \"auto\",\n",
    "    \"gradient_clipping\": 1.0,\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 2,\n",
    "        \"offload_optimizer\": {\n",
    "            \"device\": \"cpu\"\n",
    "        },\n",
    "        \"contiguous_gradients\": true,\n",
    "        \"overlap_comm\": true\n",
    "    },\n",
    "    \"fp16\": {\n",
    "        \"enabled\": \"auto\",\n",
    "        \"loss_scale\": 0,\n",
    "        \"initial_scale_power\": 16,\n",
    "        \"loss_scale_window\": 1000,\n",
    "        \"hysteresis\": 2,\n",
    "        \"min_loss_scale\": 1\n",
    "    },\n",
    "    \"bf16\": {\n",
    "        \"enabled\": \"auto\"\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"AdamW\",\n",
    "        \"params\": {\n",
    "            \"lr\": \"auto\",\n",
    "            \"betas\": [0.9, 0.999],\n",
    "            \"eps\": 1e-8,\n",
    "            \"weight_decay\": \"auto\"\n",
    "        }\n",
    "    },\n",
    "    \"scheduler\": {\n",
    "        \"type\": \"WarmupDecayLR\",\n",
    "        \"params\": {\n",
    "            \"warmup_min_lr\": \"auto\",\n",
    "            \"warmup_max_lr\": \"auto\",\n",
    "            \"warmup_num_steps\": \"auto\",\n",
    "            \"total_num_steps\": \"auto\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "EOL\n",
    "\n",
    "echo \"Starting Stage 1: Initial Fine-tuning\"\n",
    "CUDA_VISIBLE_DEVICES=0,1,2,3 \\\n",
    "python -m torch.distributed.run \\\n",
    "    --nproc_per_node=4 \\\n",
    "    -m llmtuner.cli.run_with_config initial_finetuning_config.yaml\n",
    "\n",
    "echo \"Starting Stage 2: Enhanced Fine-tuning\"\n",
    "CUDA_VISIBLE_DEVICES=0,1,2,3 \\\n",
    "python -m torch.distributed.run \\\n",
    "    --nproc_per_node=4 \\\n",
    "    -m llmtuner.cli.run_with_config enhanced_finetuning_config.yaml\n",
    "\n",
    "echo \"Starting Evaluation\"\n",
    "python -m llmtuner.cli.run_with_config evaluation_config.yaml\n",
    "\n",
    "echo \"Fine-tuning process complete!\"\n",
    "\"\"\"\n",
    "\n",
    "# Write to file\n",
    "with open(\"run_icd10_finetuning.sh\", \"w\") as f:\n",
    "    f.write(full_training_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provides the complete configuration for our two-stage fine-tuning approach using LLaMA-Factory:\n",
    "\n",
    "1. Initial fine-tuning establishes comprehensive ICD-10 code knowledge\n",
    "2. Enhanced fine-tuning adapts the model to handle linguistic and lexical variations\n",
    "\n",
    "The configuration files and scripts can be adapted for different model sizes or variations of the Llama model family. The approach is optimized for both performance and memory efficiency using DeepSpeed and distributed training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
