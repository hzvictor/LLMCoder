{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICD-10 Two-Stage Fine-Tuning Process\n",
    "\n",
    "This notebook implements the two-stage fine-tuning approach for medical coding automation:\n",
    "1. Initial fine-tuning with complete ICD-10 code set\n",
    "2. Enhanced fine-tuning for handling linguistic and lexical variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "import deepspeed\n",
    "import json\n",
    "from datasets import Dataset\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Choose between proprietary (GPT-4o mini) and open-source (Llama) models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model selection - choose between proprietary and open-source models\n",
    "MODEL_TYPE = \"llama\"  # Options: \"llama\" or \"gpt4o_mini\"\n",
    "\n",
    "# Configuration\n",
    "if MODEL_TYPE == \"llama\":\n",
    "    BASE_MODEL = \"meta-llama/Llama-3.2-1B\"  # Options: Llama-3.2-1B, Llama-3.2-3B, or Llama-3.1-8B\n",
    "    USE_LORA = True  # For memory efficiency with Llama models\n",
    "else:\n",
    "    BASE_MODEL = \"gpt-4o-mini\"  # For OpenAI API calls\n",
    "    USE_LORA = False\n",
    "    \n",
    "# Paths\n",
    "initial_output_dir = \"./icd10_initial_model\"\n",
    "enhanced_output_dir = \"./icd10_enhanced_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading Function\n",
    "\n",
    "Function to load ICD-10 data from JSONL format with system, user, and assistant messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_icd10_training_data(jsonl_path):\n",
    "    \"\"\"Load ICD-10 dataset from JSONL format with system, user, assistant messages\"\"\"\n",
    "    with open(jsonl_path, 'r') as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    \n",
    "    # Extract messages from JSONL format\n",
    "    formatted_data = []\n",
    "    for item in data:\n",
    "        messages = item[\"messages\"]\n",
    "        system_content = next(msg[\"content\"] for msg in messages if msg[\"role\"] == \"system\")\n",
    "        user_content = next(msg[\"content\"] for msg in messages if msg[\"role\"] == \"user\")\n",
    "        assistant_content = next(msg[\"content\"] for msg in messages if msg[\"role\"] == \"assistant\")\n",
    "        \n",
    "        formatted_data.append({\n",
    "            \"system\": system_content,\n",
    "            \"user\": user_content,\n",
    "            \"assistant\": assistant_content\n",
    "        })\n",
    "    \n",
    "    return Dataset.from_list(formatted_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Initial Fine-Tuning\n",
    "\n",
    "The first stage of fine-tuning uses the complete ICD-10 code set (74,260 code-description pairs) to provide the model with comprehensive medical coding knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_initial_finetuning(train_data_path, output_dir, epochs=10):\n",
    "    \"\"\"Initial fine-tuning with the complete ICD-10 code set\"\"\"\n",
    "    \n",
    "    if MODEL_TYPE == \"gpt4o_mini\":\n",
    "        # OpenAI API-based fine-tuning\n",
    "        from openai import OpenAI\n",
    "        client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "        \n",
    "        # Start fine-tuning job\n",
    "        response = client.fine_tuning.jobs.create(\n",
    "            training_file=train_data_path,\n",
    "            model=\"gpt-4o-mini\",\n",
    "            hyperparameters={\n",
    "                \"n_epochs\": epochs,\n",
    "                \"learning_rate_multiplier\": 1.8\n",
    "            }\n",
    "        )\n",
    "        print(f\"Fine-tuning job created: {response.id}\")\n",
    "        return response.id\n",
    "    else:\n",
    "        # Hugging Face-based fine-tuning for Llama models\n",
    "        train_dataset = load_icd10_training_data(train_data_path)\n",
    "        \n",
    "        # Initialize tokenizer and model\n",
    "        tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            BASE_MODEL,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        if USE_LORA:\n",
    "            # Use LoRA for parameter-efficient fine-tuning\n",
    "            model = prepare_model_for_kbit_training(model)\n",
    "            lora_config = LoraConfig(\n",
    "                r=16,  # rank\n",
    "                lora_alpha=32,  # scaling factor\n",
    "                target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "                lora_dropout=0.05,\n",
    "                bias=\"none\",\n",
    "            )\n",
    "            model = get_peft_model(model, lora_config)\n",
    "        \n",
    "        # Training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            num_train_epochs=epochs,\n",
    "            per_device_train_batch_size=1,\n",
    "            gradient_accumulation_steps=8,\n",
    "            learning_rate=1e-5,\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            warmup_steps=100,\n",
    "            logging_steps=10,\n",
    "            save_strategy=\"epoch\",\n",
    "            save_total_limit=2,\n",
    "            fp16=True,\n",
    "            report_to=\"tensorboard\",\n",
    "            deepspeed=\"ds_config.json\" if torch.cuda.device_count() > 1 else None\n",
    "        )\n",
    "        \n",
    "        # Initialize trainer\n",
    "        trainer = SFTTrainer(\n",
    "            model=model,\n",
    "            train_dataset=train_dataset,\n",
    "            args=training_args,\n",
    "            tokenizer=tokenizer,\n",
    "            max_seq_length=50,  # Maximum token length for ICD-10 descriptions\n",
    "            dataset_text_field=\"text\"\n",
    "        )\n",
    "        \n",
    "        # Start training\n",
    "        trainer.train()\n",
    "        trainer.save_model(os.path.join(output_dir, \"final_model\"))\n",
    "        return os.path.join(output_dir, \"final_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: Enhanced Fine-Tuning\n",
    "\n",
    "The second stage focuses on improving the model's ability to handle linguistic and lexical variations in clinical documentation, including:\n",
    "- Reordered diagnostic expressions\n",
    "- Medical abbreviations\n",
    "- Typographical errors\n",
    "- Multiple concurrent conditions\n",
    "- Sentences with embedded diagnostic information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_enhanced_finetuning(initial_model_path, variation_data_paths, output_dir):\n",
    "    \"\"\"\n",
    "    Enhanced fine-tuning to handle linguistic and lexical variations\n",
    "    \n",
    "    Args:\n",
    "        initial_model_path: Path to the initially fine-tuned model\n",
    "        variation_data_paths: Dictionary mapping variation types to data paths\n",
    "        output_dir: Directory to save the enhanced model\n",
    "    \"\"\"\n",
    "    if MODEL_TYPE == \"gpt4o_mini\":\n",
    "        from openai import OpenAI\n",
    "        client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "        \n",
    "        # Start fine-tuning jobs for each variation type\n",
    "        job_ids = {}\n",
    "        for variation_type, data_path in variation_data_paths.items():\n",
    "            response = client.fine_tuning.jobs.create(\n",
    "                training_file=data_path,\n",
    "                model=initial_model_path,  # Use the initially fine-tuned model\n",
    "                hyperparameters={\n",
    "                    \"n_epochs\": 5,  # Fewer epochs for enhanced tuning\n",
    "                    \"learning_rate_multiplier\": 1.0  # Lower learning rate\n",
    "                }\n",
    "            )\n",
    "            job_ids[variation_type] = response.id\n",
    "            print(f\"Enhanced fine-tuning job for {variation_type} created: {response.id}\")\n",
    "        return job_ids\n",
    "    else:\n",
    "        # Initialize model from initial fine-tuning\n",
    "        tokenizer = AutoTokenizer.from_pretrained(initial_model_path, trust_remote_code=True)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            initial_model_path,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # Process each variation type sequentially\n",
    "        for variation_type, data_path in variation_data_paths.items():\n",
    "            print(f\"Starting enhanced fine-tuning for {variation_type}\")\n",
    "            \n",
    "            # Load variation-specific data\n",
    "            variation_dataset = load_icd10_training_data(data_path)\n",
    "            \n",
    "            # Training arguments with lower learning rate and fewer epochs\n",
    "            training_args = TrainingArguments(\n",
    "                output_dir=os.path.join(output_dir, variation_type),\n",
    "                num_train_epochs=5,\n",
    "                per_device_train_batch_size=1,\n",
    "                gradient_accumulation_steps=8,\n",
    "                learning_rate=5e-6,  # Lower learning rate for enhanced tuning\n",
    "                lr_scheduler_type=\"cosine\",\n",
    "                warmup_steps=50,\n",
    "                logging_steps=10,\n",
    "                save_strategy=\"epoch\",\n",
    "                save_total_limit=1,\n",
    "                fp16=True,\n",
    "                report_to=\"tensorboard\",\n",
    "                deepspeed=\"ds_config.json\" if torch.cuda.device_count() > 1 else None\n",
    "            )\n",
    "            \n",
    "            # Initialize trainer\n",
    "            trainer = SFTTrainer(\n",
    "                model=model,\n",
    "                train_dataset=variation_dataset,\n",
    "                args=training_args,\n",
    "                tokenizer=tokenizer,\n",
    "                max_seq_length=100,  # Longer context for variations\n",
    "                dataset_text_field=\"text\"\n",
    "            )\n",
    "            \n",
    "            # Fine-tune on this variation\n",
    "            trainer.train()\n",
    "            \n",
    "            # Save intermediate model after each variation\n",
    "            model_path = os.path.join(output_dir, f\"{variation_type}_model\")\n",
    "            trainer.save_model(model_path)\n",
    "            \n",
    "            # Update model to continue with the next variation\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "        \n",
    "        # Save final enhanced model\n",
    "        final_model_path = os.path.join(output_dir, \"final_enhanced_model\")\n",
    "        model.save_pretrained(final_model_path)\n",
    "        tokenizer.save_pretrained(final_model_path)\n",
    "        return final_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Complete Two-Stage Process\n",
    "\n",
    "Now let's run both stages to train the model for medical coding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Stage 1: Initial Fine-Tuning...\n"
     ]
    }
   ],
   "source": [
    "# Define data paths\n",
    "icd10_complete_path = \"icd10_complete_codes.jsonl\"  # Path to complete ICD-10 dataset\n",
    "\n",
    "# Variation datasets for enhanced fine-tuning\n",
    "variation_datasets = {\n",
    "    \"reordered\": \"icd10_reordered_descriptions.jsonl\",\n",
    "    \"abbreviations\": \"icd10_medical_abbreviations.jsonl\",\n",
    "    \"typos\": \"icd10_typographical_errors.jsonl\",\n",
    "    \"multiple_conditions\": \"icd10_multiple_conditions.jsonl\",\n",
    "    \"sentence_embedding\": \"icd10_sentence_embedding.jsonl\"\n",
    "}\n",
    "\n",
    "print(\"Starting Stage 1: Initial Fine-Tuning...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial fine-tuning complete! Model saved at: ./icd10_initial_model/final_model\n"
     ]
    }
   ],
   "source": [
    "# This cell would actually run the initial fine-tuning\n",
    "# In a notebook environment, you might want to skip actual execution unless needed\n",
    "\n",
    "# Comment out the next line if you want to skip actual execution\n",
    "# initial_model = run_initial_finetuning(icd10_complete_path, initial_output_dir, epochs=10)\n",
    "\n",
    "# For demonstration, we'll simulate completion\n",
    "initial_model = os.path.join(initial_output_dir, \"final_model\")\n",
    "print(f\"Initial fine-tuning complete! Model saved at: {initial_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Stage 2: Enhanced Fine-Tuning...\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting Stage 2: Enhanced Fine-Tuning...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced fine-tuning complete! Final model saved at: ./icd10_enhanced_model/final_enhanced_model\n"
     ]
    }
   ],
   "source": [
    "# This cell would actually run the enhanced fine-tuning\n",
    "# Comment out the next line if you want to skip actual execution\n",
    "# enhanced_model = run_enhanced_finetuning(initial_model, variation_datasets, enhanced_output_dir)\n",
    "\n",
    "# For demonstration, we'll simulate completion\n",
    "enhanced_model = os.path.join(enhanced_output_dir, \"final_enhanced_model\")\n",
    "print(f\"Enhanced fine-tuning complete! Final model saved at: {enhanced_model}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
