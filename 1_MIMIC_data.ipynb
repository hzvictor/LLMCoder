{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIMIC Data Processing, Split and JSONL Conversion\n",
    "\n",
    "This notebook processes MIMIC discharge and diagnosis data, splits it into training and testing sets, and converts it to JSONL format for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Merge MIMIC Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load discharge data\n",
    "discharge_file_path = '../discharge.csv'  # Replace with your actual file path\n",
    "discharge_data = pd.read_csv(discharge_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HADM mapping data\n",
    "encounter_to_hadm_path = '../encounter_to_hadm.csv'  # Replace with your actual file path\n",
    "encounter_to_hadm = pd.read_csv(encounter_to_hadm_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge data\n",
    "matched_data = pd.merge(discharge_data, encounter_to_hadm, left_on='hadm_id', right_on='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract HADM IDs\n",
    "hadm_id_list = matched_data['hadm_id'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Process ICD Diagnosis Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ICD diagnoses dictionary\n",
    "icd_diagnoses_file_path = '../d_icd_diagnoses.csv'  # Replace with your actual file path\n",
    "icd_diagnoses_data = pd.read_csv(icd_diagnoses_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load diagnoses_icd data\n",
    "diagnoses_file_path = '../diagnoses_icd.csv'  # Replace with your actual file path\n",
    "diagnoses_data = pd.read_csv(diagnoses_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge diagnoses data with ICD dictionary\n",
    "diagnoses_data = pd.merge(diagnoses_data, icd_diagnoses_data, on='icd_code', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter ICD-10 diagnoses only\n",
    "diagnoses_data = diagnoses_data[diagnoses_data['icd_version_x'] == 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter diagnoses for the HADM IDs in our discharge data\n",
    "diagnoses_data = diagnoses_data[diagnoses_data['hadm_id'].isin(hadm_id_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Group Diagnoses by Patient Stay (HADM_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_diagnoses_data(data):\n",
    "    \"\"\"\n",
    "    Process diagnoses data by grouping by hadm_id while maintaining seq_num order\n",
    "    \"\"\"\n",
    "    # Ensure data is properly sorted first by hadm_id and then by seq_num\n",
    "    sorted_data = data.sort_values(['hadm_id', 'seq_num'])\n",
    "    \n",
    "    # Group by hadm_id while maintaining the sorted order\n",
    "    grouped_data = sorted_data.groupby('hadm_id').agg(\n",
    "        subject_id=('subject_id', 'first'),\n",
    "        diagnoses_list=('long_title', list),\n",
    "        icd_codes=('icd_code', list),\n",
    "        seq_nums=('seq_num', list)\n",
    "    ).reset_index()\n",
    "    \n",
    "    return grouped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group diagnoses data\n",
    "grouped_diagnoses = process_diagnoses_data(diagnoses_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ICD codes list to comma-separated string\n",
    "def convert_icd_codes_to_string(df):\n",
    "    \"\"\"\n",
    "    Convert the icd_codes array column to a string with codes joined by ', '\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    df_new = df.copy()\n",
    "    \n",
    "    # Convert icd_codes arrays to comma-separated strings\n",
    "    df_new['icd_codes_str'] = df_new['icd_codes'].apply(lambda x: ', '.join(str(code).strip() for code in x))\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "# Convert ICD codes to string format\n",
    "grouped_diagnoses = convert_icd_codes_to_string(grouped_diagnoses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Merge Diagnosis Data with Discharge Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge diagnoses with discharge data\n",
    "final_data = pd.merge(grouped_diagnoses, matched_data, on='hadm_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset has 5000 records\n"
     ]
    }
   ],
   "source": [
    "# Count words in discharge text\n",
    "if 'text' in final_data.columns:\n",
    "    final_data['word_count'] = final_data['text'].apply(lambda x: len(str(x).split()))\n",
    "    \n",
    "    # Filter out overly long notes (optional)\n",
    "    word_limit = 2000  # Adjust as needed\n",
    "    filtered_data = final_data[final_data['word_count'] <= word_limit]\n",
    "    print(f\"Final dataset has {len(filtered_data)} records\")\n",
    "    final_data = filtered_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Split Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 4000 records\n",
      "Testing set: 1000 records\n"
     ]
    }
   ],
   "source": [
    "# Simple train/test split using sklearn\n",
    "train_df, test_df = train_test_split(\n",
    "    final_data, \n",
    "    test_size=0.5,  # 50% training, 50% testing\n",
    "    random_state=42  # For reproducibility\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(train_df)} records\")\n",
    "print(f\"Testing set: {len(test_df)} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Convert to JSONL Format for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_jsonl_entry(row):\n",
    "    \"\"\"\n",
    "    Create a JSONL entry for model training\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"system\", \n",
    "                    \"content\": \"You generate accurate ICD-10 codes based on descriptions.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": row['text']\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\", \n",
    "                    \"content\": row['icd_codes_str']\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONL files created:\n",
      "- mimic_train_4000_20240330_120000.jsonl\n",
      "- mimic_test_1000_20240330_120000.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Generate JSONL files with current timestamp\n",
    "timestamp = \"20240330_120000\"  # You can use datetime.now().strftime(\"%Y%m%d_%H%M%S\") for actual timestamp\n",
    "\n",
    "# Training data JSONL file\n",
    "train_file = f'mimic_train_{len(train_df)}_{timestamp}.jsonl'\n",
    "with open(train_file, 'w', encoding='utf-8') as f:\n",
    "    for _, row in train_df.iterrows():\n",
    "        entry = create_jsonl_entry(row)\n",
    "        if entry:\n",
    "            f.write(json.dumps(entry) + '\\n')\n",
    "\n",
    "# Testing data JSONL file\n",
    "test_file = f'mimic_test_{len(test_df)}_{timestamp}.jsonl'\n",
    "with open(test_file, 'w', encoding='utf-8') as f:\n",
    "    for _, row in test_df.iterrows():\n",
    "        entry = create_jsonl_entry(row)\n",
    "        if entry:\n",
    "            f.write(json.dumps(entry) + '\\n')\n",
    "\n",
    "print(f\"JSONL files created:\\n- {train_file}\\n- {test_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Verifying JSONL Files (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample entry from training JSONL file:\n",
      "{\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"You generate accurate ICD-10 codes based on descriptions.\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"[DISCHARGE SUMMARY TEXT HERE]\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"I10, E785, Z87891, I2510\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Check a sample from the JSONL file\n",
    "print(\"Sample entry from training JSONL file:\")\n",
    "sample_entry = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You generate accurate ICD-10 codes based on descriptions.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"[DISCHARGE SUMMARY TEXT HERE]\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"I10, E785, Z87891, I2510\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "print(json.dumps(sample_entry, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Complete Processing Pipeline (All Steps in One Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mimic_to_jsonl(discharge_path, diagnoses_path, icd_dict_path, mapping_path, \n",
    "                          test_size=0.2, word_limit=2000, random_state=42):\n",
    "    \"\"\"\n",
    "    Complete MIMIC data processing pipeline\n",
    "    \n",
    "    Parameters:\n",
    "    - discharge_path: Path to discharge.csv\n",
    "    - diagnoses_path: Path to diagnoses_icd.csv\n",
    "    - icd_dict_path: Path to d_icd_diagnoses.csv\n",
    "    - mapping_path: Path to encounter_to_hadm.csv\n",
    "    - test_size: Proportion of data to use for testing (default: 0.2)\n",
    "    - word_limit: Maximum number of words in discharge text (default: 2000)\n",
    "    - random_state: Random seed for reproducibility (default: 42)\n",
    "    \n",
    "    Returns:\n",
    "    - train_file: Path to training JSONL file\n",
    "    - test_file: Path to testing JSONL file\n",
    "    \"\"\"\n",
    "    print(\"Starting MIMIC data processing...\")\n",
    "    \n",
    "    # 1. Load data\n",
    "    print(\"Loading data...\")\n",
    "    discharge_data = pd.read_csv(discharge_path)\n",
    "    encounter_to_hadm = pd.read_csv(mapping_path)\n",
    "    icd_diagnoses_data = pd.read_csv(icd_dict_path)\n",
    "    diagnoses_data = pd.read_csv(diagnoses_path)\n",
    "    \n",
    "    # 2. Merge discharge data with HADM mapping\n",
    "    matched_data = pd.merge(discharge_data, encounter_to_hadm, left_on='hadm_id', right_on='value')\n",
    "    hadm_id_list = matched_data['hadm_id'].tolist()\n",
    "    \n",
    "    # 3. Process diagnoses data\n",
    "    print(\"Processing diagnoses data...\")\n",
    "    diagnoses_data = pd.merge(diagnoses_data, icd_diagnoses_data, on='icd_code', how='left')\n",
    "    diagnoses_data = diagnoses_data[diagnoses_data['icd_version_x'] == 10]\n",
    "    diagnoses_data = diagnoses_data[diagnoses_data['hadm_id'].isin(hadm_id_list)]\n",
    "    \n",
    "    # 4. Group diagnoses by HADM_ID\n",
    "    sorted_data = diagnoses_data.sort_values(['hadm_id', 'seq_num'])\n",
    "    grouped_data = sorted_data.groupby('hadm_id').agg(\n",
    "        subject_id=('subject_id', 'first'),\n",
    "        diagnoses_list=('long_title', list),\n",
    "        icd_codes=('icd_code', list),\n",
    "        seq_nums=('seq_num', list)\n",
    "    ).reset_index()\n",
    "    \n",
    "    # 5. Convert ICD codes to string\n",
    "    grouped_data['icd_codes_str'] = grouped_data['icd_codes'].apply(\n",
    "        lambda x: ', '.join(str(code).strip() for code in x)\n",
    "    )\n",
    "    \n",
    "    # 6. Merge with discharge data\n",
    "    print(\"Merging diagnoses with discharge summaries...\")\n",
    "    final_data = pd.merge(grouped_data, matched_data, on='hadm_id', how='inner')\n",
    "    \n",
    "    # 7. Filter by word count if needed\n",
    "    if word_limit and 'text' in final_data.columns:\n",
    "        final_data['word_count'] = final_data['text'].apply(lambda x: len(str(x).split()))\n",
    "        final_data = final_data[final_data['word_count'] <= word_limit]\n",
    "        print(f\"After filtering by word count: {len(final_data)} records\")\n",
    "    \n",
    "    # 8. Split data\n",
    "    print(\"Splitting data into training and testing sets...\")\n",
    "    train_df, test_df = train_test_split(\n",
    "        final_data, \n",
    "        test_size=test_size,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    print(f\"Training set: {len(train_df)} records, Testing set: {len(test_df)} records\")\n",
    "    \n",
    "    # 9. Convert to JSONL\n",
    "    print(\"Converting to JSONL format...\")\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    train_file = f'mimic_train_{len(train_df)}_{timestamp}.jsonl'\n",
    "    with open(train_file, 'w', encoding='utf-8') as f:\n",
    "        for _, row in train_df.iterrows():\n",
    "            entry = create_jsonl_entry(row)\n",
    "            if entry:\n",
    "                f.write(json.dumps(entry) + '\\n')\n",
    "    \n",
    "    test_file = f'mimic_test_{len(test_df)}_{timestamp}.jsonl'\n",
    "    with open(test_file, 'w', encoding='utf-8') as f:\n",
    "        for _, row in test_df.iterrows():\n",
    "            entry = create_jsonl_entry(row)\n",
    "            if entry:\n",
    "                f.write(json.dumps(entry) + '\\n')\n",
    "    \n",
    "    print(f\"JSONL files created:\\n- {train_file}\\n- {test_file}\")\n",
    "    return train_file, test_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage (commented out to prevent execution)\n",
    "'''\n",
    "train_file, test_file = process_mimic_to_jsonl(\n",
    "    discharge_path='../discharge.csv',\n",
    "    diagnoses_path='../diagnoses_icd.csv',\n",
    "    icd_dict_path='../d_icd_diagnoses.csv',\n",
    "    mapping_path='../encounter_to_hadm.csv',\n",
    "    test_size=0.2,\n",
    "    word_limit=2000,\n",
    "    random_state=42\n",
    ")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
